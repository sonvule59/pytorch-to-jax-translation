[
  {
    "id": "e1.py",
    "language": "python",
    "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Generate synthetic data\ntorch.manual_seed(42)\nX = torch.rand(100, 1) * 10  # 100 data points between 0 and 10\ny = 2 * X + 3 + torch.randn(100, 1)  # Linear relationship with noise\n\n# Define the Linear Regression Model\nclass LinearRegressionModel(nn.Module):\n    def __init__(self):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = nn.Linear(1, 1)  # Single input and single output\n\n    def forward(self, x):\n        return self.linear(x)\n\n# Initialize the model, loss function, and optimizer\nmodel = LinearRegressionModel()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 1000\nfor epoch in range(epochs):\n    # Forward pass\n    predictions = model(X)\n    loss = criterion(predictions, y)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Log progress every 100 epochs\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\n# Display the learned parameters\n[w, b] = model.linear.parameters()\nprint(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n\n# Testing on new data\nX_test = torch.tensor([[4.0], [7.0]])\nwith torch.no_grad():\n    predictions = model(X_test)\n    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")",
    "translated_code": "import { jax, jax.numpy, jax.nn as jnn } from '@jax/jax-virtual-memory@18.7'\n\n# Generate synthetic data\njax.random.seed(42)\nX = jax.random.uniform(jax.numpy.float32(0), jax.numpy.float32(10), 100)\ny = 2 * X + 3 + jax.random.normal(jax.numpy.float32(0), jax.numpy.float32(1), 100)\n\n# Define the Linear Regression Model\nclass LinearRegressionModel(jnn.Module):\n    def __init__(self):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = jnn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.linear(x)\n\n# Initialize the model, loss function, and optimizer\nmodel = LinearRegressionModel()\ncriterion = jax.mse_loss\noptimizer = jax.optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 1000\nfor epoch in range(epochs):\n    # Forward pass\n    predictions = model(X)\n    loss = criterion(predictions, y)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Log progress every 100 epochs\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\n# Display the learned parameters\n[w, b] = model.linear.parameters()\nprint(f\"Learned weight: {w.value():.4f}, Learned bias: {b.value():.4f}\")\n\n# Testing on new data\nX_test = jax.numpy.array([[4.0], [7.0]])\nwith jax.no_op:\n    predictions = model(X_test)\n    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")"
  },
  {
    "id": "e2.py",
    "language": "python",
    "code": "import torch\nimport pandas as pd\n\ntorch.manual_seed(42)\nX = torch.rand(100, 1) * 10  # 100 data points between 0 and 10\ny = 2 * X + 3 + torch.randn(100, 1)  # Linear relationship with noise\n\n# Save the generated data to data.csv\ndata = torch.cat((X, y), dim=1)\ndf = pd.DataFrame(data.numpy(), columns=['X', 'y'])\ndf.to_csv('data.csv', index=False)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\n\nclass LinearRegressionDataset(Dataset):\n    def __init__(self, csv_file):\n        # Load data from CSV file\n        self.data = pd.read_csv(csv_file)\n        self.X = torch.tensor(self.data['X'].values, dtype=torch.float32).view(-1, 1)\n        self.y = torch.tensor(self.data['y'].values, dtype=torch.float32).view(-1, 1)\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Example usage of the DataLoader\ndataset = LinearRegressionDataset('data.csv')\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Define the Linear Regression Model\nclass LinearRegressionModel(nn.Module):\n    def __init__(self):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = nn.Linear(1, 1)  # Single input and single output\n\n    def forward(self, x):\n        return self.linear(x)\n\n# Initialize the model, loss function, and optimizer\nmodel = LinearRegressionModel()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 1000\nfor epoch in range(epochs):\n    for batch_X, batch_y in dataloader:\n        # Forward pass\n        predictions = model(batch_X)\n        loss = criterion(predictions, batch_y)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Log progress every 100 epochs\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\n# Display the learned parameters\n[w, b] = model.linear.parameters()\nprint(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n\n# Testing on new data\nX_test = torch.tensor([[4.0], [7.0]])\nwith torch.no_grad():\n    predictions = model(X_test)\n    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")",
    "translated_code": "JAX 20.12.7\nJAX Config: \n# jaxconfig.json\n{ \"jax\": \"tracing\", \"load_jax_config_from_file\": false, \"jax_max_n_stack\": 16, \"jax_enable_x64_floating_point\": true }\n\n\n<think>\nOkay, I need to convert the given Python code using PyTorch to JAX. Let me go through each part step by step.\n\nFirst, the original code starts by importing PyTorch and generating some data with torch. In JAX, the equivalent would be using jax.data.array or jax.numpy for creating random data. Also, jax.manual_seed is used instead of torch.manual_seed.\n\nThe data creation part: X is random between 0 and 10. In JAX, I can use jax.numpy.random.rand to generate the array. Then scale it. Similarly, y is 2*X +3 plus some noise. The noise can be generated with jax.numpy.random.normal. So I'll replace all torch operations with JAX equivalents here.\n\nSaving to CSV: The code uses pandas to save the data. Since JAX has its own pandas interface, maybe using jax.numpy.to_array and then converting to a DataFrame. But the original code reads the CSV when creating the dataset. However, in the JAX code provided, the data is saved to data.csv using pandas as well. Wait, but in the JAX code example given, the user might have to adjust that part. Wait, the original code in the PyTorch section saves the data to data.csv using pandas. So in the JAX code, the same code would be used, but perhaps the data is generated in JAX. Wait, no, the user's code example includes both PyTorch and JAX code. Wait, looking back, the user provided a code that first uses PyTorch, then after some comments, another section that uses JAX. Wait, the original code they provided has a mix of PyTorch and JAX code? Let me check again.\n\nWait, the user's input code starts with PyTorch, then after some code, there's another section that starts with \"Import torch\" again, but then uses JAX imports. That might be a mistake. But perhaps the user intended to show the PyTorch code and then the JAX code. However, the task is to convert the PyTorch code to JAX. So perhaps the entire code, including the data saving part, needs to be converted.\n\nBut looking at the user's code, the first part is PyTorch code that generates data, saves it to CSV, then the second part is JAX code that defines a dataset and model. Wait, maybe the user's code is a combination, but the actual task is to convert the PyTorch parts to JAX.\n\nWait, the user's instruction says: \"Translate the following Python code using PyTorch to JAX. Output only valid JAX code.\" So the entire code provided should be converted to JAX. But looking at the code, there's a mix of PyTorch and JAX code. For example, the first part uses PyTorch to generate data and save to CSV. Then, after some empty comments, another section that uses JAX for dataset and model. That might be a mistake in the user's code. But perhaps the user intended to show the code in PyTorch and then the JAX code separately, but the actual task is to convert the entire code to JAX.\n\nAlternatively, maybe the user's code is a combination, but the correct approach is to convert all PyTorch parts to JAX. Let me parse the code step by step.\n\nOriginal code steps:\n\n1. Import torch and pandas.\n2. Set manual seed.\n3. Generate X and y with torch.\n4. Save data to CSV.\n5. Then, another section that imports torch again (probably a mistake), defines a dataset and dataloader using PyTorch's Dataset and DataLoader.\n6. Defines a model, trains it, etc.\n\nBut the task is to convert this entire code to JAX. So the JAX code should replace all PyTorch parts.\n\nBut in the user's code, after the first data generation, there's another section that starts with \"Import torch\" again, but then uses JAX imports. That's probably a mistake. The second part is actually the JAX code, but the user might have mixed up the code. So perhaps the correct approach is to process the entire code, replacing PyTorch with JAX.\n\nBut let's look at the code the user provided. The first part is PyTorch code, then after some comments, another code block that starts with \"Import torch\" again, but then uses JAX functions. That's conflicting. Maybe the user intended to show the code in two parts: first PyTorch, then JAX. But the task is to convert the entire code to"
  },
  {
    "id": "e3.py",
    "language": "python",
    "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom matplotlib import pyplot as plt\n\n# Generate synthetic data\ntorch.manual_seed(42)\nX = torch.rand(100, 1) * 10  # 100 data points between 0 and 10\ny = 2 * X + 3 + torch.randn(100, 1)  # Linear relationship with noise\n\n# Define the Linear Regression Model within a CustomActivationModel class\nclass CustomActivationModel(nn.Module):\n    def __init__(self):\n        super(CustomActivationModel, self).__init__()\n        self.linear = nn.Linear(1, 1)  # Single input and single output\n\n    def custom_activation(self, x):\n        return torch.tanh(x) + x\n\n    def forward(self, x):\n        return self.custom_activation(self.linear(x))\n\n# Initialize the model, loss function, and optimizer\nmodel = CustomActivationModel()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 1000\nfor epoch in range(epochs):\n    # Forward pass\n    predictions = model(X)\n    loss = criterion(predictions, y)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Log progress every 100 epochs\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\n# Display the learned parameters\n[w, b] = model.linear.parameters()\nprint(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n\n# Plot the model fit to the train data\nplt.figure(figsize=(4, 4))\nplt.scatter(X, y, label='Training Data')\nplt.plot(X, w.item()*X + b.item(), 'r', label='Model Fit')\nplt.legend()\nplt.show()\n\n# Testing on new data\nX_test = torch.tensor([[4.0], [7.0]])\nwith torch.no_grad():\n    predictions = model(X_test)\n    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n\n    ",
    "translated_code": "import jax.numpy as jnp\nfrom jax import multivariate_normal as jmvn\nfrom jax.nn import LinearRegression\nfrom jax.optim import MinimalGradientDescent\n\n# Generate synthetic data\njax.random.seed(42)\nX = jnp.random.uniform(0, 10, (100, 1))\ny = 2 * X + 3 + jnp.random.normal(0, 0.2, (100, 1))\n\n# Define the Linear Regression Model\nmodel = LinearRegression(input_shape=(1,))\n\n# Initialize parameters using jax.random.normal\ninitial_params = jnp.zeros((1,))\noptimizer = MinimalGradientDescent(initial_params, jax.lru_cache(max_size=100))\n\n# Training loop with JAX automatically handling gradients\nfor i in range(1000):\n    # Forward and backward pass\n    optimizer.apply(jax.fn.vectorize(model.apply)(X))[0]\n    loss = optimizer.loss\n\n    # Minimize the loss\n    optimizer.minimize(loss)\n\n    # Log every 100 epochs\n    if i + 1 % 100 == 0:\n        print(f\"Epoch [{i + 1}/{1000}], Loss: {loss.item():.4f}\")\n\n# Get learned parameters\nweight, bias = optimizer.get_params()[0]\nprint(f\"Learned weight: {weight.item():.4f}, Learned bias: {bias.item():.4f}\")\n\n# Plotting (Note: JAX doesn't have built-in plotting; use a JAX-compatible plot library)\n# For demonstration, using jax.plot.jaxplot (if available) or similar\n# This is a placeholder for actual plotting code"
  },
  {
    "id": "e4.py",
    "language": "python",
    "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Generate synthetic data\ntorch.manual_seed(42)\nX = torch.rand(100, 1) * 10  # 100 data points between 0 and 10\ny = 2 * X + 3 + torch.randn(100, 1)  # Linear relationship with noise\n\n\nclass HuberLoss(nn.Module):\n    def __init__(self, delta=1.0):\n        super(HuberLoss, self).__init__()\n        self.delta = delta\n    \n    def forward(self, y_pred, y_true):\n        # Calculate the absolute error\n        error = torch.abs(y_pred - y_true)\n        \n        # Apply the Huber loss formula\n        loss = torch.where(error <= self.delta,\n                           0.5 * error**2,  # L2 loss for small errors\n                           self.delta * (error - 0.5 * self.delta))  # L1 loss for large errors\n        return loss.mean()  # Return the mean loss across all samples\n\n\n# Define the Linear Regression Model\nclass LinearRegressionModel(nn.Module):\n    def __init__(self):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = nn.Linear(1, 1)  # Single input and single output\n\n    def forward(self, x):\n        return self.linear(x)\n\n# Initialize the model, loss function, and optimizer\nmodel = LinearRegressionModel()\ncriterion = HuberLoss(delta=1.0)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 1000\nfor epoch in range(epochs):\n    # Forward pass\n    predictions = model(X)\n    loss = criterion(predictions, y)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Log progress every 100 epochs\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\n# Display the learned parameters\n[w, b] = model.linear.parameters()\nprint(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n\n# Testing on new data\nX_test = torch.tensor([[4.0], [7.0]])\nwith torch.no_grad():\n    predictions = model(X_test)\n    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")",
    "translated_code": "JAX code"
  },
  {
    "id": "e5.py",
    "language": "python",
    "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Generate synthetic data\ntorch.manual_seed(42)\nX = torch.rand(100, 2) * 10  # 100 data points with 2 features\ny = (X[:, 0] + X[:, 1] * 2).unsqueeze(1) + torch.randn(100, 1)  # Non-linear relationship with noise\n\n# Define the Deep Neural Network Model\nclass DNNModel(nn.Module):\n    def __init__(self):\n        super(DNNModel, self).__init__()\n        self.fc1 = nn.Linear(2, 10)  # Input layer to hidden layer\n        self.relu = nn.ReLU()       # Activation function\n        self.fc2 = nn.Linear(10, 1) # Hidden layer to output layer\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n    \n# Initialize the model, loss function, and optimizer\nmodel = DNNModel()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 1000\nfor epoch in range(epochs):\n    # Forward pass\n    predictions = model(X)\n    loss = criterion(predictions, y)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Log progress every 100 epochs\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\n# Testing on new data\nX_test = torch.tensor([[4.0, 3.0], [7.0, 8.0]])\nwith torch.no_grad():\n    predictions = model(X_test)\n    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n    ",
    "translated_code": "18.7+ (JAX Viper/Underlying 0.15.25)\n---\n\n\n\n<think>\nOkay, I need to convert the given PyTorch code to JAX. Let me start by looking at each part of the code.\n\nFirst, the imports. The original uses torch and related modules. In JAX, I should replace torch with jax.jax. But wait, JAX doesn't use torch, so maybe just import jax and use jax.numpy for the arrays. However, the user might have set up a JAX configuration already, so maybe just import the necessary modules. But the original code has torch.manual_seed, which in JAX would be jax.random.seed or jax.numpy.random.seed. Let me check.\n\nThe data generation part: X is created with torch.rand, which in JAX would be jax.numpy.random.rand. Similarly, y uses torch.randn. Also, the model's forward method uses nn.Linear, which in JAX would be jax.nn.Linear. Oh right, JAX has its own modules, so I should use jax.nn.Linear instead of torch.nn.Linear.\n\nThe optimizer: In PyTorch, optim.Adam is used. In JAX, the equivalent is jax.optim.Adam. So the optimizer line would be jax.optim.Adam(model.parameters(), lr=0.01).\n\nThe training loop: The forward pass is model(X), which in JAX would be similar, but since JAX uses functional APIs, maybe the model's forward is already compatible. The loss calculation uses MSELoss, which in JAX is jax.nn.mse_loss. So the criterion would be jax.nn.mse_loss.\n\nAlso, the logging part: The print statement uses.item(), which in JAX functions, the outputs are already in JAX arrays, but when printed, they might show JAX tensors. However, the user probably just wants the code to be compatible, so the syntax remains the same.\n\nWait, in JAX, the data is handled with jax.numpy arrays, so X and y should be jax.numpy arrays. So replacing torch tensors with jax.numpy arrays. For example, X = jax.numpy.random.rand(100, 2) * 10. Similarly for y.\n\nThe model definition: The DNNModel uses nn.Linear, which becomes jax.nn.Linear. So in the __init__ method, self.fc1 = jax.nn.Linear(2, 10), etc.\n\nThe optimizer is initialized with jax.optim.Adam. The loss function is jax.nn.mse_loss, so criterion = jax.nn.mse_loss. Also, the MSELoss in PyTorch is replaced by JAX's mse_loss.\n\nIn the training loop, the forward pass is model(X), which should work as is if the model is a JAX-compatible module. The backward pass uses optimizer.zero_grad(), which is the same. The loss.backward() and optimizer.step() are the same syntax.\n\nTesting part: X_test is a tensor, which in JAX is a jax.numpy array. The model's forward is called, and predictions are made. The print statement would output the JAX tensors, but the code structure remains the same.\n\nSo putting it all together:\n\nReplace all torch imports with jax imports where necessary. Use jax.numpy for data creation. Use jax.nn for modules. Use jax.optim for the optimizer. Use jax.nn.mse_loss for the loss function.\n\nWait, but in JAX, the MSELoss is available via jax.nn.mse_loss, so the criterion line becomes criterion = jax.nn.mse_loss(predictions, y).\n\nAlso, the data generation uses torch.randn, which becomes jax.numpy.random.randn.\n\nAnother thing: In JAX, the random seed can be set using jax.random.seed(42) instead of torch.manual_seed. So the initial data generation would use jax.random.seed(42) instead of torch.manual_seed(42).\n\nSo the first line would be jax.random.seed(42), then X = jax.numpy.random.rand(100, 2) * 10.\n\nSimilarly for y: y = (X[:,0] + X[:,1]*2).unsqueeze(1) + jax.numpy.random.randn(100,1).\n\nThe model's forward method remains the same, but uses jax.nn.Linear.\n\nThe optimizer is jax.optim.Adam(model.parameters(), lr=0.01). Wait, in PyTorch, model.parameters() is a generator, same in JAX. So that's okay.\n\nThe loss is computed with jax.nn.mse_loss, which takes predictions and y as inputs.\n\nIn the training loop, the variables like loss.item() would not be needed because JAX functions return JAX scalars, and.item() is used when converting to Python scalars. But in JAX, when using jax.grad or other functions, the outputs are already JAX arrays. However, in the code"
  }
]